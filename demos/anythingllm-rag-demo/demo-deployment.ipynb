{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AnythingLLM RAG Demo - Deployment\n",
        "\n",
        "This notebook is an interactive walkthrough to deploy the AnythingLLM RAG demo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before starting, ensure you have completed the main platform setup:\n",
        "- ✓ OpenShift GitOps installed\n",
        "- ✓ RHOAI installed and DataScienceCluster ready\n",
        "- ✓ GPU nodes deployed\n",
        "- ✓ At least one model downloaded and deployed\n",
        "\n",
        "See the [platform-deployment.ipynb](../../platform-deployment.ipynb) notebook for these steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "echo \"=== GitOps Status ===\"\n",
        "oc get pods -n openshift-gitops | grep -E 'NAME|server'\n",
        "\n",
        "echo -e \"\\n=== RHOAI Status ===\"\n",
        "oc get datasciencecluster -A\n",
        "\n",
        "echo -e \"\\n=== GPU Nodes ===\"\n",
        "oc get nodes -l nvidia.com/gpu.present=true\n",
        "\n",
        "echo -e \"\\n=== Model Storage ===\"\n",
        "oc get pvc -n demo\n",
        "\n",
        "echo -e \"\\n=== InferenceServices ===\"\n",
        "oc get inferenceservice -n demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configure Model Selection\n",
        "\n",
        "Select which model AnythingLLM should use for LLM inference and embeddings.\n",
        "\n",
        "**Available Models:**\n",
        "\n",
        "| Model ID | Service Name | Model Display Name | Description |\n",
        "|----------|--------------|-------------------|-------------|\n",
        "| `qwen3-vl-8b` | `qwen3-vl-8b-predictor` | `Qwen3-VL-8B-Instruct` | Multimodal vision-language model (recommended) |\n",
        "| `granite-7b` | `granite-7b-predictor` | `granite-7b-instruct` | IBM's open instruction model |\n",
        "| `llama-3-8b` | `llama-3-8b-predictor` | `llama-3-8b-instruct` | Meta's Llama 3 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Model: qwen3-vl-8b-fp8\n",
            "Display Name: qwen3-vl-8b-fp8\n",
            "Service URL: http://qwen3-vl-8b-fp8-predictor.demo.svc.cluster.local/v1\n"
          ]
        }
      ],
      "source": [
        "# Select your model (must match one deployed in prerequisites)\n",
        "MODEL = \"qwen3-vl-8b-fp8\"\n",
        "NAMESPACE = \"demo\"\n",
        "\n",
        "# Model display names for AnythingLLM\n",
        "MODEL_DISPLAY_NAMES = {\n",
        "    \"qwen3-vl-8b\": \"Qwen3-VL-8B-Instruct\",\n",
        "    \"granite-7b\": \"granite-7b-instruct\",\n",
        "    \"llama-3-8b\": \"llama-3-8b-instruct\"\n",
        "}\n",
        "\n",
        "# Build service URL\n",
        "MODEL_DISPLAY_NAME = MODEL_DISPLAY_NAMES.get(MODEL, MODEL)\n",
        "SERVICE_URL = f\"http://{MODEL}-predictor.{NAMESPACE}.svc.cluster.local/v1\"\n",
        "\n",
        "print(f\"Selected Model: {MODEL}\")\n",
        "print(f\"Display Name: {MODEL_DISPLAY_NAME}\")\n",
        "print(f\"Service URL: {SERVICE_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Deploy AnythingLLM\n",
        "\n",
        "Deploy AnythingLLM via GitOps with the selected model configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "application.argoproj.io/anythingllm created\n"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$SERVICE_URL\" \"$MODEL_DISPLAY_NAME\"\n",
        "# Generate and apply ArgoCD Application with model configuration\n",
        "cat <<EOF | oc apply -f -\n",
        "apiVersion: argoproj.io/v1alpha1\n",
        "kind: Application\n",
        "metadata:\n",
        "  name: anythingllm\n",
        "  namespace: openshift-gitops\n",
        "  labels:\n",
        "    app.kubernetes.io/name: anythingllm\n",
        "    app.kubernetes.io/part-of: rhoai-demos\n",
        "    app.kubernetes.io/component: application\n",
        "spec:\n",
        "  project: default\n",
        "  source:\n",
        "    repoURL: https://github.com/redhat-ai-americas/rhoai-app-demos.git\n",
        "    targetRevision: main\n",
        "    path: apps/3rd-party-apps/anythingllm/helm\n",
        "    helm:\n",
        "      values: |\n",
        "        namespace: anythingllm\n",
        "        image:\n",
        "          repository: mintplexlabs/anythingllm\n",
        "          tag: latest\n",
        "          pullPolicy: IfNotPresent\n",
        "        replicas: 1\n",
        "        service:\n",
        "          type: ClusterIP\n",
        "          port: 3001\n",
        "        resources:\n",
        "          requests:\n",
        "            cpu: \"500m\"\n",
        "            memory: \"1Gi\"\n",
        "          limits:\n",
        "            cpu: \"2\"\n",
        "            memory: \"4Gi\"\n",
        "        storage:\n",
        "          size: 10Gi\n",
        "        llm:\n",
        "          provider: \"generic-openai\"\n",
        "          baseUrl: \"$1\"\n",
        "          model: \"$2\"\n",
        "          apiKey: \"\"\n",
        "        embedding:\n",
        "          provider: \"generic-openai\"\n",
        "          baseUrl: \"$1\"\n",
        "          model: \"$2\"\n",
        "          apiKey: \"\"\n",
        "        env:\n",
        "          - name: LLM_PROVIDER\n",
        "            value: \"generic-openai\"\n",
        "          - name: GENERIC_OPEN_AI_BASE_PATH\n",
        "            value: \"$1\"\n",
        "          - name: GENERIC_OPEN_AI_MODEL_PREF\n",
        "            value: \"$2\"\n",
        "          - name: GENERIC_OPEN_AI_MODEL_TOKEN_LIMIT\n",
        "            value: \"8192\"\n",
        "          - name: EMBEDDING_ENGINE\n",
        "            value: \"generic-openai\"\n",
        "          - name: EMBEDDING_BASE_PATH\n",
        "            value: \"$1\"\n",
        "          - name: EMBEDDING_MODEL_PREF\n",
        "            value: \"$2\"\n",
        "          - name: VECTOR_DB\n",
        "            value: \"chroma\"\n",
        "          - name: CHROMA_ENDPOINT\n",
        "            value: \"http://chromadb:8000\"\n",
        "          - name: DISABLE_TELEMETRY\n",
        "            value: \"true\"\n",
        "        chromadb:\n",
        "          enabled: true\n",
        "          image:\n",
        "            repository: chromadb/chroma\n",
        "            tag: latest\n",
        "          resources:\n",
        "            requests:\n",
        "              cpu: \"250m\"\n",
        "              memory: \"512Mi\"\n",
        "            limits:\n",
        "              cpu: \"1\"\n",
        "              memory: \"2Gi\"\n",
        "          storage:\n",
        "            size: 20Gi\n",
        "  destination:\n",
        "    server: https://kubernetes.default.svc\n",
        "    namespace: anythingllm\n",
        "  syncPolicy:\n",
        "    automated:\n",
        "      selfHeal: true\n",
        "      prune: true\n",
        "    syncOptions:\n",
        "      - CreateNamespace=true\n",
        "    retry:\n",
        "      limit: 5\n",
        "      backoff:\n",
        "        duration: 10s\n",
        "        factor: 2\n",
        "        maxDuration: 5m\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wait for Deployment (2-3 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Wait for ArgoCD to sync the application\n",
        "echo \"Waiting for ArgoCD Application to sync...\"\n",
        "sleep 10  # Give ArgoCD a moment to detect the application\n",
        "\n",
        "# Wait for ArgoCD sync (this may take 2-3 minutes)\n",
        "oc wait --for=jsonpath='{.status.sync.status}'=Synced \\\n",
        "  application/anythingllm -n openshift-gitops --timeout=300s\n",
        "\n",
        "# Wait for pods to be ready\n",
        "echo -e \"\\nWaiting for AnythingLLM pods to be ready...\"\n",
        "oc wait --for=condition=Ready pod \\\n",
        "  -l app=anythingllm \\\n",
        "  -n anythingllm --timeout=180s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get AnythingLLM URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Get the route URL\n",
        "ANYTHINGLLM_URL=$(oc get route anythingllm -n anythingllm -o jsonpath='https://{.spec.host}')\n",
        "echo \"AnythingLLM URL: $ANYTHINGLLM_URL\"\n",
        "echo \"\"\n",
        "echo \"✓ AnythingLLM is ready! Open the URL above in your browser.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Verify Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "echo \"=== AnythingLLM Pods ===\"\n",
        "oc get pods -n anythingllm\n",
        "\n",
        "echo -e \"\\n=== AnythingLLM Route ===\"\n",
        "oc get route anythingllm -n anythingllm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Model Connectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$SERVICE_URL\"\n",
        "# Test that AnythingLLM can reach the model service\n",
        "echo \"Testing connectivity to model service: $1\"\n",
        "oc exec -it deploy/anythingllm -n anythingllm -- \\\n",
        "  curl -s $1/models | head -20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "✓ AnythingLLM is deployed and configured!\n",
        "\n",
        "Continue to [demo-user-instructions.md](./demo-user-instructions.md) to:\n",
        "- Set up your first workspace\n",
        "- Upload documents for RAG\n",
        "- Test the chat interface\n",
        "\n",
        "## Cleanup\n",
        "\n",
        "To remove AnythingLLM when done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Uncomment to delete AnythingLLM\n",
        "# oc delete application anythingllm -n openshift-gitops\n",
        "# oc delete project anythingllm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### AnythingLLM pod not starting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Check pod status and logs\n",
        "oc get pods -n anythingllm\n",
        "echo -e \"\\n=== AnythingLLM Logs ===\"\n",
        "oc logs -l app=anythingllm -n anythingllm --tail=50\n",
        "echo -e \"\\n=== ChromaDB Logs ===\"\n",
        "oc logs -l app=chromadb -n anythingllm --tail=50\n",
        "echo -e \"\\n=== Pod Description ===\"\n",
        "oc describe pod -l app=anythingllm -n anythingllm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model connection error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$MODEL\" \"$NAMESPACE\"\n",
        "# Verify the InferenceService is Ready\n",
        "echo \"=== InferenceService Status ===\"\n",
        "oc get inferenceservice $1 -n $2\n",
        "\n",
        "echo -e \"\\n=== Test model endpoint directly ===\"\n",
        "oc run curl-test --image=curlimages/curl -it --rm -n $2 -- \\\n",
        "  curl -s http://$1-predictor.$2.svc.cluster.local/v1/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
