{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RHOAI Platform Deployment\n",
        "\n",
        "This notebook is an interactive walkthrough to deploy the full RHOAI platform stack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install OpenShift GitOps\n",
        "\n",
        "**Important**: The GitOps operator must be installed directly (not via ArgoCD) since ArgoCD doesn't exist yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "namespace/openshift-gitops created\n",
            "subscription.operators.coreos.com/openshift-gitops-operator created\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Step 1: Install the operator subscription\n",
        "oc apply -k platform/gitops-operator/base/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deployment.apps/openshift-gitops-operator-controller-manager condition met\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Wait for the operator to install (1-2 minutes)\n",
        "oc wait --for=condition=Available deployment/openshift-gitops-operator-controller-manager \\\n",
        "  -n openshift-operators --timeout=300s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-admin created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: resource argocds/openshift-gitops is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by oc apply. oc apply should only be used on resources created declaratively by either oc create --save-config or oc apply. The missing annotation will be patched automatically.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "argocd.argoproj.io/openshift-gitops configured\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Step 2: Create the ArgoCD instance\n",
        "oc apply -k platform/gitops-operator/instance/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pod/openshift-gitops-server-674758b98b-f7p2l condition met\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Wait for ArgoCD server to be ready (1-2 minutes)\n",
        "oc wait --for=condition=Ready pod -l app.kubernetes.io/name=openshift-gitops-server \\\n",
        "  -n openshift-gitops --timeout=300s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Install RHOAI\n",
        "\n",
        "RHOAI 3.x requires dependencies to be installed first. The GitOps Application will handle this automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "application.argoproj.io/rhoai-operator unchanged\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# This deploys:\n",
        "# - Node Feature Discovery (NFD) operator\n",
        "# - Red Hat Build for Kueue operator  \n",
        "# - RHOAI operator subscription (fast-3.x channel)\n",
        "# - DataScienceCluster instance\n",
        "oc apply -f gitops/platform/rhoai-operator.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wait about 5-10 minutes for the resources to install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No resources found\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Verify the DataScienceCluster instance is ready\n",
        "oc get dsc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install NVIDIA GPU Operator\n",
        "\n",
        "**Required for GPU support.** The NVIDIA GPU Operator enables GPU workload scheduling by installing drivers, device plugins, and monitoring tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "application.argoproj.io/nvidia-gpu-operator created\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Deploy the NVIDIA GPU Operator\n",
        "oc apply -f gitops/platform/nvidia-gpu-operator.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wait 2-5 minutes for the operator to install and create the ClusterPolicy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'admin:login' logged in successfully\n",
            "Context 'openshift-gitops-server-openshift-gitops.apps.cluster-5rtpd.5rtpd.sandbox3531.opentlc.com' updated\n",
            "application.argoproj.io/gpu-machineset-aws-g6 created\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Verify the GPU operator is installed\n",
        "oc get csv -n nvidia-gpu-operator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Deploy GPU Nodes (AWS)\n",
        "\n",
        "**Most demos require GPU nodes for model inference.** Deploy GPU infrastructure before running demos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "\n",
        "# Get cluster name from infrastructure object\n",
        "infra_result = subprocess.run(\n",
        "    [\"oc\", \"get\", \"infrastructure\", \"cluster\", \"-o\", \"json\"],\n",
        "    capture_output=True, text=True, check=True\n",
        ")\n",
        "infra_data = json.loads(infra_result.stdout)\n",
        "\n",
        "CLUSTER_NAME = infra_data[\"status\"][\"infrastructureName\"]\n",
        "INFRA_ID = infra_data[\"status\"][\"infrastructureName\"]\n",
        "REGION = infra_data[\"status\"][\"platformStatus\"][\"aws\"][\"region\"]\n",
        "\n",
        "# Get first available worker machine to determine AZ\n",
        "machines_result = subprocess.run(\n",
        "    [\"oc\", \"get\", \"machines\", \"-n\", \"openshift-machine-api\", \"-l\", \"machine.openshift.io/cluster-api-machine-role=worker\", \"-o\", \"json\"],\n",
        "    capture_output=True, text=True, check=True\n",
        ")\n",
        "machines_data = json.loads(machines_result.stdout)\n",
        "AVAILABILITY_ZONE = machines_data[\"items\"][0][\"spec\"][\"providerSpec\"][\"value\"][\"placement\"][\"availabilityZone\"]\n",
        "\n",
        "# Get AMI ID from an existing worker MachineSet\n",
        "machineset_result = subprocess.run(\n",
        "    [\"oc\", \"get\", \"machineset\", \"-n\", \"openshift-machine-api\", \"-o\", \"json\"],\n",
        "    capture_output=True, text=True, check=True\n",
        ")\n",
        "machineset_data = json.loads(machineset_result.stdout)\n",
        "# Find a non-GPU machineset and get its AMI ID\n",
        "AMI_ID = \"\"\n",
        "for item in machineset_data[\"items\"]:\n",
        "    if \"gpu\" not in item[\"metadata\"][\"name\"]:\n",
        "        AMI_ID = item[\"spec\"][\"template\"][\"spec\"][\"providerSpec\"][\"value\"][\"ami\"][\"id\"]\n",
        "        if AMI_ID:  # Skip if empty\n",
        "            break\n",
        "\n",
        "print(f\"Cluster Name: {CLUSTER_NAME}\")\n",
        "print(f\"Region: {REGION}\")\n",
        "print(f\"Availability Zone: {AVAILABILITY_ZONE}\")\n",
        "print(f\"Infrastructure ID: {INFRA_ID}\")\n",
        "print(f\"AMI ID: {AMI_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4a: Deploy GPU MachineSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$CLUSTER_NAME\" \"$REGION\" \"$AVAILABILITY_ZONE\" \"$INFRA_ID\" \"$AMI_ID\"\n",
        "# Deploy the GPU MachineSet ArgoCD Application with cluster-specific values\n",
        "# For AWS g6.2xlarge (1x NVIDIA L4, 8 vCPU, 32GB RAM, ~$1.10/hr)\n",
        "\n",
        "# Get ArgoCD admin password\n",
        "ARGOCD_PASSWORD=$(oc get secret/openshift-gitops-cluster -n openshift-gitops -o jsonpath='{.data.admin\\.password}' | base64 -d)\n",
        "ARGOCD_SERVER=$(oc get route openshift-gitops-server -n openshift-gitops -o jsonpath='{.spec.host}')\n",
        "\n",
        "# Login to ArgoCD CLI\n",
        "argocd login $ARGOCD_SERVER --username admin --password $ARGOCD_PASSWORD --insecure\n",
        "\n",
        "# Create the Application (without auto-sync)\n",
        "oc apply -f gitops/infra/gpu-machineset-aws-g6.yaml\n",
        "\n",
        "# Set Helm parameters with cluster values\n",
        "argocd app set gpu-machineset-aws-g6 \\\n",
        "  -p clusterName=\"$1\" \\\n",
        "  -p region=\"$2\" \\\n",
        "  -p availabilityZone=\"$3\" \\\n",
        "  -p infraID=\"$4\" \\\n",
        "  -p amiId=\"$5\" > /dev/null 2>&1\n",
        "\n",
        "# Now enable auto-sync and sync\n",
        "argocd app set gpu-machineset-aws-g6 --sync-policy automated --auto-prune --self-heal > /dev/null 2>&1\n",
        "argocd app sync gpu-machineset-aws-g6 > /dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "node/ip-10-0-29-70.us-east-2.compute.internal condition met\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Wait for GPU node to be Ready (5-10 minutes)\n",
        "# The GPU operator daemonsets will also deploy to this node\n",
        "oc wait --for=condition=Ready nodes -l nvidia.com/gpu.present=true --timeout=600s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4b: Verify GPU Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cluster-5rtpd-6dtsz-gpu-us-east-2a      1         1         1       1           6m\n",
            "cluster-5rtpd-6dtsz-gpu-us-east-2a-29q86      Running   g6.2xlarge    us-east-2   us-east-2a   6m\n",
            "NAME                                       STATUS   ROLES        AGE     VERSION\n",
            "ip-10-0-29-70.us-east-2.compute.internal   Ready    gpu,worker   2m45s   v1.33.6\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Check MachineSet was created\n",
        "oc get machineset -n openshift-machine-api | grep gpu\n",
        "\n",
        "# Check Machine is provisioning\n",
        "oc get machine -n openshift-machine-api | grep gpu\n",
        "\n",
        "# Verify GPU node is Ready and has GPU resources available\n",
        "oc get nodes -l nvidia.com/gpu.present=true\n",
        "\n",
        "# Verify GPU is allocatable (should show nvidia.com/gpu: 1)\n",
        "oc get node -l nvidia.com/gpu.present=true -o json | jq '.items[0].status.allocatable'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Download and Deploy Models\n",
        "\n",
        "**Most demos require at least one model to be deployed.** Choose and deploy the models you need to support specific demos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5a: Create HuggingFace Token Secret (optional for some models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Get HuggingFace token from user input\n",
        "# (look for input prompt (in VSCode it's above))\n",
        "HF_TOKEN = getpass.getpass(\"Enter HuggingFace token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$HF_TOKEN\"\n",
        "oc create namespace demo --dry-run=client -o yaml | oc apply -f -\n",
        "oc create secret generic huggingface-token \\\n",
        "  --from-literal=token=$1 \\\n",
        "  -n demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5b: Download Model (10-30 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Model Choices:**\n",
        "\n",
        "- **`qwen3-vl-8b`** - Multimodal vision-language model (~18GB, recommended for RAG demos)\n",
        "- **`granite-7b`** - IBM's open instruction model (~14GB)\n",
        "- **`llama-3-8b`** - Meta's Llama 3 (~16GB, requires HuggingFace license acceptance)\n",
        "\n",
        "Select the model you want to deploy by setting the `MODEL` variable in the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a model to download from the choices above\n",
        "MODEL = \"qwen3-vl-8b\"\n",
        "NAMESPACE = \"demo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "application.argoproj.io/model-qwen3-vl-8b-pvc configured\n"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$MODEL\"\n",
        "# Deploy the model download job via GitOps\n",
        "oc apply -f gitops/platform/models/$1-pvc.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\n",
            "qwen3-vl-8b-model-storage   Bound    pvc-53ddba1f-b193-42bf-bc9a-6a60b13cfcde   18Gi       RWO            gp3-csi        <unset>                 125m\n"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$NAMESPACE\"\n",
        "# Verify PVC was created and is bound\n",
        "oc get pvc -n $1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5c: Deploy Model Serving (3-5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "application.argoproj.io/model-qwen3-vl-8b-serving created\n"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$MODEL\"\n",
        "# Deploy the model serving via GitOps\n",
        "oc apply -f gitops/platform/models/$1-serving.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inferenceservice.serving.kserve.io/qwen3-vl-8b patched\n"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$MODEL\" \"$NAMESPACE\"\n",
        "oc patch inferenceservice $1 -n $2 \\\n",
        "  --type=merge -p '{\"metadata\":{\"annotations\":{\"serving.kserve.io/stop\":\"false\"}}}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error from server (NotFound): applications.app.k8s.io \"model-qwen3-vl-8b-serving\" not found\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Process was interrupted.\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command 'b'# Wait for ArgoCD application to be ready\\noc wait --for=condition=Ready application/model-$1-serving \\\\\\n  -n openshift-gitops --timeout=180s\\n\\n# Wait for InferenceService to be ready (5-10 minutes for GPU node scheduling and model loading)\\noc wait --for=condition=Ready inferenceservice/$1 \\\\\\n  -n $2 --timeout=600s\\n'' returned non-zero exit status 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-s \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$MODEL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$NAMESPACE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m# Wait for ArgoCD application to be ready\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43moc wait --for=condition=Ready application/model-$1-serving \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -n openshift-gitops --timeout=180s\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Wait for InferenceService to be ready (5-10 minutes for GPU node scheduling and model loading)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43moc wait --for=condition=Ready inferenceservice/$1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -n $2 --timeout=600s\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/Red Hat/redhat-ai-americas/rhoai-app-demos/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/Red Hat/redhat-ai-americas/rhoai-app-demos/.venv/lib/python3.12/site-packages/IPython/core/magics/script.py:160\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    159\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/Red Hat/redhat-ai-americas/rhoai-app-demos/.venv/lib/python3.12/site-packages/IPython/core/magics/script.py:339\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError while terminating subprocess (pid=\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (p.pid, e))\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(p.returncode, cell) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[31mCalledProcessError\u001b[39m: Command 'b'# Wait for ArgoCD application to be ready\\noc wait --for=condition=Ready application/model-$1-serving \\\\\\n  -n openshift-gitops --timeout=180s\\n\\n# Wait for InferenceService to be ready (5-10 minutes for GPU node scheduling and model loading)\\noc wait --for=condition=Ready inferenceservice/$1 \\\\\\n  -n $2 --timeout=600s\\n'' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "%%bash -s \"$MODEL\" \"$NAMESPACE\"\n",
        "# Wait for ArgoCD application to be ready\n",
        "oc wait --for=condition=Ready application/model-$1-serving \\\n",
        "  -n openshift-gitops --timeout=180s\n",
        "\n",
        "# Wait for InferenceService to be ready (5-10 minutes for GPU node scheduling and model loading)\n",
        "oc wait --for=condition=Ready inferenceservice/$1 \\\n",
        "  -n $2 --timeout=600s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$MODEL\" \"$NAMESPACE\"\n",
        "# Get the inference endpoint\n",
        "INFERENCE_URL=$(oc get inferenceservice $1 -n $2 -o jsonpath='{.status.url}')\n",
        "echo \"Inference URL: $INFERENCE_URL\"\n",
        "\n",
        "# Test the model\n",
        "oc run curl-test --image=curlimages/curl -it --rm -n $2 -- \\\n",
        "  curl -X POST http://$1-predictor.$2.svc.cluster.local/v1/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d \"{\\\"model\\\": \\\"$1\\\", \\\"prompt\\\": \\\"Hello\\\", \\\"max_tokens\\\": 50}\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
