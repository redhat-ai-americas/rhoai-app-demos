{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RHOAI Platform Deployment\n",
        "\n",
        "This notebook is an interactive walkthrough to deploy the full RHOAI platform stack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install OpenShift GitOps\n",
        "\n",
        "**Important**: The GitOps operator must be installed directly (not via ArgoCD) since ArgoCD doesn't exist yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "namespace/openshift-gitops created\n",
            "subscription.operators.coreos.com/openshift-gitops-operator created\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Step 1: Install the operator subscription\n",
        "oc apply -k platform/gitops-operator/base/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deployment.apps/openshift-gitops-operator-controller-manager condition met\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Wait for the operator to install (1-2 minutes)\n",
        "oc wait --for=condition=Available deployment/openshift-gitops-operator-controller-manager \\\n",
        "  -n openshift-operators --timeout=300s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clusterrolebinding.rbac.authorization.k8s.io/openshift-gitops-admin created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: resource argocds/openshift-gitops is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by oc apply. oc apply should only be used on resources created declaratively by either oc create --save-config or oc apply. The missing annotation will be patched automatically.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "argocd.argoproj.io/openshift-gitops configured\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Step 2: Create the ArgoCD instance\n",
        "oc apply -k platform/gitops-operator/instance/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pod/openshift-gitops-server-674758b98b-f7p2l condition met\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Wait for ArgoCD server to be ready (1-2 minutes)\n",
        "oc wait --for=condition=Ready pod -l app.kubernetes.io/name=openshift-gitops-server \\\n",
        "  -n openshift-gitops --timeout=300s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Install RHOAI\n",
        "\n",
        "RHOAI 3.x requires dependencies to be installed first. The GitOps Application will handle this automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "application.argoproj.io/rhoai-operator unchanged\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# This deploys:\n",
        "# - Node Feature Discovery (NFD) operator\n",
        "# - Red Hat Build for Kueue operator  \n",
        "# - RHOAI operator subscription (fast-3.x channel)\n",
        "# - DataScienceCluster instance\n",
        "oc apply -f gitops/platform/rhoai-operator.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wait about 5-10 minutes for the resources to install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No resources found\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Verify the DataScienceCluster instance is ready\n",
        "oc get dsc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install NVIDIA GPU Operator\n",
        "\n",
        "**Required for GPU support.** The NVIDIA GPU Operator enables GPU workload scheduling by installing drivers, device plugins, and monitoring tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "application.argoproj.io/nvidia-gpu-operator created\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Deploy the NVIDIA GPU Operator\n",
        "oc apply -f gitops/platform/nvidia-gpu-operator.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wait 2-5 minutes for the operator to install and create the ClusterPolicy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME                                DISPLAY                            VERSION   REPLACES                            PHASE\n",
            "gpu-operator-certified.v24.9.2      NVIDIA GPU Operator                24.9.2    gpu-operator-certified.v24.9.1      Succeeded\n",
            "kueue-operator.v1.2.0               Red Hat build of Kueue             1.2.0                                         Succeeded\n",
            "openshift-gitops-operator.v1.19.0   Red Hat OpenShift GitOps           1.19.0    openshift-gitops-operator.v1.18.2   Succeeded\n",
            "rhods-operator.3.2.0                Red Hat OpenShift AI               3.2.0     rhods-operator.3.0.0                Succeeded\n",
            "servicemeshoperator3.v3.1.0         Red Hat OpenShift Service Mesh 3   3.1.0     servicemeshoperator3.v3.0.7         Succeeded\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Verify the GPU operator is installed\n",
        "oc get csv -n nvidia-gpu-operator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Deploy GPU Nodes (AWS)\n",
        "\n",
        "**Most demos require GPU nodes for model inference.** Deploy GPU infrastructure before running demos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4a: Configure GPU Node Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Node Configuration\n",
        "# Adjust these values based on your needs\n",
        "\n",
        "# Instance type for GPU nodes\n",
        "# Common options: g6.2xlarge (1x L4), g6.4xlarge (1x L4), g5.2xlarge (1x A10G)\n",
        "INSTANCE_TYPE = \"g6.4xlarge\"\n",
        "\n",
        "# Root volume size in GB\n",
        "ROOT_VOLUME_SIZE = 120\n",
        "\n",
        "# Volume type: gp3 (newer, better performance) or gp2 (legacy)\n",
        "ROOT_VOLUME_TYPE = \"gp2\"\n",
        "\n",
        "# IOPS for gp3 volumes (ignored for gp2)\n",
        "ROOT_VOLUME_IOPS = 3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster Name: cluster-5rtpd-6dtsz\n",
            "Region: us-east-2\n",
            "Availability Zone: us-east-2a\n",
            "Infrastructure ID: cluster-5rtpd-6dtsz\n",
            "AMI ID: ami-0bc8dda494f111572\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import json\n",
        "\n",
        "# Get cluster name from infrastructure object\n",
        "infra_result = subprocess.run(\n",
        "    [\"oc\", \"get\", \"infrastructure\", \"cluster\", \"-o\", \"json\"],\n",
        "    capture_output=True, text=True, check=True\n",
        ")\n",
        "infra_data = json.loads(infra_result.stdout)\n",
        "\n",
        "CLUSTER_NAME = infra_data[\"status\"][\"infrastructureName\"]\n",
        "INFRA_ID = infra_data[\"status\"][\"infrastructureName\"]\n",
        "REGION = infra_data[\"status\"][\"platformStatus\"][\"aws\"][\"region\"]\n",
        "\n",
        "# Get first available worker machine to determine AZ\n",
        "machines_result = subprocess.run(\n",
        "    [\"oc\", \"get\", \"machines\", \"-n\", \"openshift-machine-api\", \"-l\", \"machine.openshift.io/cluster-api-machine-role=worker\", \"-o\", \"json\"],\n",
        "    capture_output=True, text=True, check=True\n",
        ")\n",
        "machines_data = json.loads(machines_result.stdout)\n",
        "AVAILABILITY_ZONE = machines_data[\"items\"][0][\"spec\"][\"providerSpec\"][\"value\"][\"placement\"][\"availabilityZone\"]\n",
        "\n",
        "# Get AMI ID from an existing worker MachineSet\n",
        "machineset_result = subprocess.run(\n",
        "    [\"oc\", \"get\", \"machineset\", \"-n\", \"openshift-machine-api\", \"-o\", \"json\"],\n",
        "    capture_output=True, text=True, check=True\n",
        ")\n",
        "machineset_data = json.loads(machineset_result.stdout)\n",
        "# Find a non-GPU machineset and get its AMI ID\n",
        "AMI_ID = \"\"\n",
        "for item in machineset_data[\"items\"]:\n",
        "    if \"gpu\" not in item[\"metadata\"][\"name\"]:\n",
        "        AMI_ID = item[\"spec\"][\"template\"][\"spec\"][\"providerSpec\"][\"value\"][\"ami\"][\"id\"]\n",
        "        if AMI_ID:  # Skip if empty\n",
        "            break\n",
        "\n",
        "print(f\"Cluster Name: {CLUSTER_NAME}\")\n",
        "print(f\"Region: {REGION}\")\n",
        "print(f\"Availability Zone: {AVAILABILITY_ZONE}\")\n",
        "print(f\"Infrastructure ID: {INFRA_ID}\")\n",
        "print(f\"AMI ID: {AMI_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4b: Deploy GPU MachineSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'admin:login' logged in successfully\n",
            "Context 'openshift-gitops-server-openshift-gitops.apps.cluster-5rtpd.5rtpd.sandbox3531.opentlc.com' updated\n",
            "application.argoproj.io/gpu-machineset-aws-g6 created\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command 'b'# Deploy the GPU MachineSet ArgoCD Application with cluster-specific values\\n\\n# Get ArgoCD admin password\\nARGOCD_PASSWORD=$(oc get secret/openshift-gitops-cluster -n openshift-gitops -o jsonpath=\\'{.data.admin\\\\.password}\\' | base64 -d)\\nARGOCD_SERVER=$(oc get route openshift-gitops-server -n openshift-gitops -o jsonpath=\\'{.spec.host}\\')\\n\\n# Login to ArgoCD CLI\\nargocd login $ARGOCD_SERVER --username admin --password $ARGOCD_PASSWORD --insecure\\n\\n# Create the Application (without auto-sync)\\noc apply -f gitops/infra/gpu-machineset-aws-g6.yaml\\n\\n# Set Helm parameters with cluster values\\nargocd app set gpu-machineset-aws-g6 \\\\\\n  -p clusterName=\"$1\" \\\\\\n  -p region=\"$2\" \\\\\\n  -p availabilityZone=\"$3\" \\\\\\n  -p infraID=\"$4\" \\\\\\n  -p amiId=\"$5\" \\\\\\n  -p rootVolume.size=\"$6\" \\\\\\n  -p rootVolume.type=\"$7\" \\\\\\n  -p rootVolume.iops=\"$8\" \\\\\\n  -p instanceType=\"$9\" > /dev/null 2>&1\\n\\n# Now enable auto-sync and sync\\nargocd app set gpu-machineset-aws-g6 --sync-policy automated --auto-prune --self-heal > /dev/null 2>&1\\nargocd app sync gpu-machineset-aws-g6 > /dev/null 2>&1\\n'' returned non-zero exit status 20.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-s \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$CLUSTER_NAME\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$REGION\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$AVAILABILITY_ZONE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$INFRA_ID\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$AMI_ID\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$ROOT_VOLUME_SIZE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$ROOT_VOLUME_TYPE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$ROOT_VOLUME_IOPS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$INSTANCE_TYPE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m# Deploy the GPU MachineSet ArgoCD Application with cluster-specific values\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Get ArgoCD admin password\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mARGOCD_PASSWORD=$(oc get secret/openshift-gitops-cluster -n openshift-gitops -o jsonpath=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m{\u001b[39;49m\u001b[33;43m.data.admin\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43m.password}\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m | base64 -d)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mARGOCD_SERVER=$(oc get route openshift-gitops-server -n openshift-gitops -o jsonpath=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m{\u001b[39;49m\u001b[33;43m.spec.host}\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Login to ArgoCD CLI\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43margocd login $ARGOCD_SERVER --username admin --password $ARGOCD_PASSWORD --insecure\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Create the Application (without auto-sync)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43moc apply -f gitops/infra/gpu-machineset-aws-g6.yaml\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Set Helm parameters with cluster values\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43margocd app set gpu-machineset-aws-g6 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -p clusterName=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -p region=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -p availabilityZone=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -p infraID=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -p amiId=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -p rootVolume.size=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$6\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -p rootVolume.type=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$7\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -p rootVolume.iops=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  -p instanceType=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$9\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m > /dev/null 2>&1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Now enable auto-sync and sync\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43margocd app set gpu-machineset-aws-g6 --sync-policy automated --auto-prune --self-heal > /dev/null 2>&1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43margocd app sync gpu-machineset-aws-g6 > /dev/null 2>&1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/Red Hat/redhat-ai-americas/rhoai-app-demos/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/Red Hat/redhat-ai-americas/rhoai-app-demos/.venv/lib/python3.12/site-packages/IPython/core/magics/script.py:160\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    159\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/Red Hat/redhat-ai-americas/rhoai-app-demos/.venv/lib/python3.12/site-packages/IPython/core/magics/script.py:348\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error \u001b[38;5;129;01mand\u001b[39;00m p.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[32m    345\u001b[39m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[32m    346\u001b[39m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[32m    347\u001b[39m     rc = p.returncode \u001b[38;5;129;01mor\u001b[39;00m -\u001b[32m9\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
            "\u001b[31mCalledProcessError\u001b[39m: Command 'b'# Deploy the GPU MachineSet ArgoCD Application with cluster-specific values\\n\\n# Get ArgoCD admin password\\nARGOCD_PASSWORD=$(oc get secret/openshift-gitops-cluster -n openshift-gitops -o jsonpath=\\'{.data.admin\\\\.password}\\' | base64 -d)\\nARGOCD_SERVER=$(oc get route openshift-gitops-server -n openshift-gitops -o jsonpath=\\'{.spec.host}\\')\\n\\n# Login to ArgoCD CLI\\nargocd login $ARGOCD_SERVER --username admin --password $ARGOCD_PASSWORD --insecure\\n\\n# Create the Application (without auto-sync)\\noc apply -f gitops/infra/gpu-machineset-aws-g6.yaml\\n\\n# Set Helm parameters with cluster values\\nargocd app set gpu-machineset-aws-g6 \\\\\\n  -p clusterName=\"$1\" \\\\\\n  -p region=\"$2\" \\\\\\n  -p availabilityZone=\"$3\" \\\\\\n  -p infraID=\"$4\" \\\\\\n  -p amiId=\"$5\" \\\\\\n  -p rootVolume.size=\"$6\" \\\\\\n  -p rootVolume.type=\"$7\" \\\\\\n  -p rootVolume.iops=\"$8\" \\\\\\n  -p instanceType=\"$9\" > /dev/null 2>&1\\n\\n# Now enable auto-sync and sync\\nargocd app set gpu-machineset-aws-g6 --sync-policy automated --auto-prune --self-heal > /dev/null 2>&1\\nargocd app sync gpu-machineset-aws-g6 > /dev/null 2>&1\\n'' returned non-zero exit status 20."
          ]
        }
      ],
      "source": [
        "%%bash -s \"$CLUSTER_NAME\" \"$REGION\" \"$AVAILABILITY_ZONE\" \"$INFRA_ID\" \"$AMI_ID\" \"$ROOT_VOLUME_SIZE\" \"$ROOT_VOLUME_TYPE\" \"$ROOT_VOLUME_IOPS\" \"$INSTANCE_TYPE\"\n",
        "# Deploy the GPU MachineSet ArgoCD Application with cluster-specific values\n",
        "\n",
        "# Get ArgoCD admin password\n",
        "ARGOCD_PASSWORD=$(oc get secret/openshift-gitops-cluster -n openshift-gitops -o jsonpath='{.data.admin\\.password}' | base64 -d)\n",
        "ARGOCD_SERVER=$(oc get route openshift-gitops-server -n openshift-gitops -o jsonpath='{.spec.host}')\n",
        "\n",
        "# Login to ArgoCD CLI\n",
        "argocd login $ARGOCD_SERVER --username admin --password $ARGOCD_PASSWORD --insecure\n",
        "\n",
        "# Create the Application (without auto-sync)\n",
        "oc apply -f gitops/infra/gpu-machineset-aws-g6.yaml\n",
        "\n",
        "# Set Helm parameters with cluster values\n",
        "argocd app set gpu-machineset-aws-g6 \\\n",
        "  -p clusterName=\"$1\" \\\n",
        "  -p region=\"$2\" \\\n",
        "  -p availabilityZone=\"$3\" \\\n",
        "  -p infraID=\"$4\" \\\n",
        "  -p amiId=\"$5\" \\\n",
        "  -p rootVolume.size=\"$6\" \\\n",
        "  -p rootVolume.type=\"$7\" \\\n",
        "  -p rootVolume.iops=\"$8\" \\\n",
        "  -p instanceType=\"$9\" > /dev/null 2>&1\n",
        "\n",
        "# Now enable auto-sync and sync\n",
        "argocd app set gpu-machineset-aws-g6 --sync-policy automated --auto-prune --self-heal > /dev/null 2>&1\n",
        "argocd app sync gpu-machineset-aws-g6 > /dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "node/ip-10-0-29-70.us-east-2.compute.internal condition met\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Wait for GPU node to be Ready (5-10 minutes)\n",
        "# The GPU operator daemonsets will also deploy to this node\n",
        "oc wait --for=condition=Ready nodes -l nvidia.com/gpu.present=true --timeout=600s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4c: Verify GPU Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cluster-5rtpd-6dtsz-gpu-us-east-2a      1         1         1       1           6m\n",
            "cluster-5rtpd-6dtsz-gpu-us-east-2a-29q86      Running   g6.2xlarge    us-east-2   us-east-2a   6m\n",
            "NAME                                       STATUS   ROLES        AGE     VERSION\n",
            "ip-10-0-29-70.us-east-2.compute.internal   Ready    gpu,worker   2m45s   v1.33.6\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Check MachineSet was created\n",
        "oc get machineset -n openshift-machine-api | grep gpu\n",
        "\n",
        "# Check Machine is provisioning\n",
        "oc get machine -n openshift-machine-api | grep gpu\n",
        "\n",
        "# Verify GPU node is Ready and has GPU resources available\n",
        "oc get nodes -l nvidia.com/gpu.present=true\n",
        "\n",
        "# Verify GPU is allocatable (should show nvidia.com/gpu: 1)\n",
        "oc get node -l nvidia.com/gpu.present=true -o json | jq '.items[0].status.allocatable'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Download and Deploy Models\n",
        "\n",
        "**Most demos require at least one model to be deployed.** Choose and deploy the models you need to support specific demos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5a: Create HuggingFace Token Secret (optional for some models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Get HuggingFace token from user input\n",
        "# (look for input prompt (in VSCode it's above))\n",
        "HF_TOKEN = getpass.getpass(\"Enter HuggingFace token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$HF_TOKEN\"\n",
        "oc create namespace demo --dry-run=client -o yaml | oc apply -f -\n",
        "oc create secret generic huggingface-token \\\n",
        "  --from-literal=token=$1 \\\n",
        "  -n demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5b: Download Model (10-30 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Model Choices:**\n",
        "\n",
        "- **`qwen3-vl-8b`** - Multimodal vision-language model (~18GB, g6.4xlarge or larger recommended)\n",
        "- **`qwen3-vl-8b-fp8`** - FP8 quantized vision-language model (~12GB, better performance, g6.2xlarge or larger)\n",
        "- **`granite-7b`** - IBM's open instruction model (~14GB)\n",
        "- **`llama-3-8b`** - Meta's Llama 3 (~16GB, requires HuggingFace license acceptance)\n",
        "\n",
        "Select the model you want to deploy by setting the `MODEL` variable in the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a model to download from the choices above\n",
        "MODEL = \"qwen3-vl-8b-fp8\"\n",
        "NAMESPACE = \"demo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "application.argoproj.io/model-qwen3-vl-8b-fp8-pvc created\n"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$MODEL\"\n",
        "# Deploy the model download job via GitOps\n",
        "oc apply -f gitops/platform/models/$1-pvc.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\n",
            "qwen3-vl-8b-model-storage   Bound    pvc-53ddba1f-b193-42bf-bc9a-6a60b13cfcde   18Gi       RWO            gp3-csi        <unset>                 125m\n"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$NAMESPACE\"\n",
        "# Verify PVC was created and is bound\n",
        "oc get pvc -n $1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5c: Deploy Model Serving (3-5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "application.argoproj.io/model-qwen3-vl-8b-fp8-serving created\n"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$MODEL\"\n",
        "# Deploy the model serving via GitOps\n",
        "oc apply -f gitops/platform/models/$1-serving.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To start the model server, patch the annotation `serving.kserve.io/stop` to `false` (To stop it, patch it back to `true`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inferenceservice.serving.kserve.io/qwen3-vl-8b-fp8 patched\n"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$MODEL\" \"$NAMESPACE\"\n",
        "oc patch inferenceservice $1 -n $2 \\\n",
        "  --type=merge -p '{\"metadata\":{\"annotations\":{\"serving.kserve.io/stop\":\"false\"}}}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME          URL   READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION   AGE\n",
            "qwen3-vl-8b         False                                                                 16h\n"
          ]
        }
      ],
      "source": [
        "%%bash -s \"$MODEL\" \"$NAMESPACE\"\n",
        "\n",
        "# Confirm the model is running and ready\n",
        "oc get inferenceservice $1 -n $2\n",
        "# Get the inference endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$MODEL\" \"$NAMESPACE\"\n",
        "# Get the inference endpoint\n",
        "INFERENCE_URL=$(oc get inferenceservice $1 -n $2 -o jsonpath='{.status.url}')\n",
        "echo \"Inference URL: $INFERENCE_URL\"\n",
        "\n",
        "# Test the model\n",
        "oc run curl-test --image=curlimages/curl -it --rm -n $2 -- \\\n",
        "  curl -X POST http://$1-predictor.$2.svc.cluster.local/v1/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d \"{\\\"model\\\": \\\"$1\\\", \\\"prompt\\\": \\\"Hello\\\", \\\"max_tokens\\\": 50}\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
